# -*- coding: utf-8 -*-
"""SINHA_FDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JF9Bcd4Up-tuZOWqMLviLC2emTIjR9qz

Data Analytics
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

df = pd.read_csv('Fda_dataset.csv')
print(df.head())

# DATA PRE-PROCESSING
#Remove Irrelevant Columns
cleaned_df = df.drop(columns=['Unnamed: 0'])
cleaned_df['Timestamp'] = pd.to_datetime(cleaned_df['Timestamp'])

print(cleaned_df.head())

# Extract Date and Time separately
cleaned_df['Date'] = cleaned_df['Timestamp'].dt.date
cleaned_df['Time'] = cleaned_df['Timestamp'].dt.time

# Drop the original 'Timestamp' column
cleaned_df = cleaned_df.drop(columns=['Timestamp'])

# Display first few rows to verify the new columns
print(cleaned_df[['Date', 'Time']].head())

# Missing values
missing_values = cleaned_df.isnull().sum()

# Display the cleaned data and missing value summary
cleaned_df.head(), missing_values

# Propose methods for handling missing values
cleaned_df['Retweets'] = cleaned_df['Retweets'].fillna(cleaned_df['Retweets'].mean())

# Verify the changes
missing_values = cleaned_df.isnull().sum()
# Display the cleaned data and missing value summary
cleaned_df.head(), missing_values

# Check for duplicates
duplicates = cleaned_df.duplicated()

# Count duplicate rows
duplicate_count = duplicates.sum()
print(f"Number of duplicate rows: {duplicate_count}")

# Remove duplicate rows
new_cleaned_df = cleaned_df.drop_duplicates()

# Verify the shape after removal
print(f"Shape after removing duplicates: {new_cleaned_df.shape}")

# Detect outliers using z-score
from scipy import stats
z_scores = np.abs(stats.zscore(new_cleaned_df[['Likes', 'Retweets',]]))
outliers = (z_scores > 3).any(axis=1)
print(f"Number of outliers: {outliers.sum()}")

# Visualization: Trend of Avg. Likes over Months/Platform
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Convert 'Date' column to datetime objects if it's not already
new_cleaned_df['Date'] = pd.to_datetime(new_cleaned_df['Date'])

# Extract the month
new_cleaned_df['Month'] = new_cleaned_df['Date'].dt.month

# Unique platforms
platforms = new_cleaned_df['Platform'].unique()

# Create a line plot for each platform in the same figure
plt.figure(figsize=(8, 6))

for platform in platforms:
    platform_data = new_cleaned_df[new_cleaned_df['Platform'] == platform]
    likes_monthly = platform_data.groupby('Month')['Likes'].mean()

    # Plot the trend for each platform
    sns.lineplot(x=likes_monthly.index, y=likes_monthly.values, marker='o', label=platform)

plt.title('Average Likes per Month for Each Platform')
plt.xlabel('Month')
plt.ylabel('Average Likes')
plt.xticks(range(1, 13))
plt.legend(title="Platform")
plt.grid(True)
plt.show()

# Scattter plot of age vs Daily Social Media Time (hrs)

plt.figure(figsize=(6, 4))
sns.scatterplot(x='Age', y='Daily Social Media Time (hrs)', data=new_cleaned_df, color='orange')
plt.title('Scatter Plot of Age vs. Daily Social Media Time')
plt.xlabel('Age')
plt.ylabel('Daily Social Media Time (hrs)')
plt.show()

# Correlation heatmap for numerical variables in the sampled dataset
numerical_features = new_cleaned_df.select_dtypes(include=["float64", "int64"])
corr_hmap = numerical_feat.corr()

plt.figure(figsize=(6, 4))
sns.heatmap(corr_hmap, annot=False, cmap="viridis",linewidth=.5,fmt='.2f')
plt.title("Correlation HeatMap")
plt.show()

# Feature Selection
# Selecting key features for clustering
clustering_features = ["Likes", "Retweets", "Daily Social Media Time (hrs)","Age"]

# Scaling the selected features using standard scaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(new_cleaned_df[clustering_features])

# Checking Elbow Method For K Means
wcss = []
k_range = range(2, 8)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow Method
plt.figure(figsize=(, 5))
plt.plot(k_range, wcss, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.show()

# K-Means Clustering
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans_labels = kmeans.fit_predict(scaled_data)

# Adding cluster labels to the sampled dataframe
new_cleaned_df["Cluster"] = kmeans_labels

# Visualizing clusters
plt.figure(figsize=(7, 5))
sns.scatterplot(x=new_cleaned_df["Daily Social Media Time (hrs)"],
                y=new_cleaned_df["Age"],hue=new_cleaned_df["Cluster"], palette="viridis")
plt.title("K-Means Clustering: Social Media Time vs Age")
plt.xlabel("Daily Social Media Time (hrs)")
plt.ylabel("Age")
plt.legend(title="Cluster")
plt.show()

#Hierarchical clustering

hierarchical_linkage = linkage(scaled_data, method="complete")
hierarchical_labels = fcluster(hierarchical_linkage, t=4, criterion="maxclust")

# Assigning clusters from hierarchical clustering
hierarchical_labels = fcluster(hierarchical_linkage, t=4, criterion="maxclust")
new_cleaned_df["Hierarchical Cluster"] = hierarchical_labels

# Plotting the dendrogram
plt.figure(figsize=(7, 5))
dendrogram(hierarchical_linkage)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()

# Accuracy: Silhouette Score
# Higher Silhouette Score indicates better-defined clusters

# Select only numerical features for silhouette score calculation
numerical_features = new_cleaned_df.select_dtypes(include=np.number)

kmeans_silhouette = silhouette_score(scaled_features, kmeans_labels) * 10
silhouette_hierarchical = silhouette_score(scaled_features, hierarchical_labels) * 10

print(f"\nAccuracy:\n")
print(f"K-Means Silhouette Score: {kmeans_silhouette:.2f}")
print(f"Silhouette Score for Hierarchical Clustering: {silhouette_hierarchical:.2f}")

# Robustness: Adjusted Rand Index (ARI) for stability
# Higher ARI indicates more stable clusters across different runs
print(f"\nRobustness:\n")
n_iterations = 10  # Number of iterations for K-Means
ari_scores = []

for i in range(n_iterations):
    kmeans_i = KMeans(n_clusters=4, random_state=i)
    labels_i = kmeans_i.fit_predict(scaled_data)
    ari = adjusted_rand_score(kmeans_labels, labels_i)
    ari_scores.append(ari)

avg_ari = np.mean(ari_scores)
print(f"Average Adjusted Rand Index (ARI): {avg_ari:.2f}")


# Number of iterations for testing
n_iterations = 10
ari_scores_hierarchical = []

# Loop through iterations
for i in range(n_iterations):
    # Perform hierarchical clustering with a different linkage matrix each time
    linkage_matrix_i = linkage(scaled_data, method='complete')
    labels_i = fcluster(linkage_matrix_i, t=4, criterion='maxclust')

    # Compare with the original hierarchical cluster labels
    ari = adjusted_rand_score(hierarchical_labels, labels_i)
    ari_scores_hierarchical.append(ari)

# Calculate and print the average ARI
avg_ari_hierarchical = np.mean(ari_scores_hierarchical)
print(f"Average Adjusted Rand Index (ARI) for Hierarchical Clustering: {avg_ari_hierarchical:.2f}")

print(f"\nSpeed:\n")
# Speed: Time taken for clustering
start_kmeans = time.time()
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans_labels = kmeans.fit_predict(scaled_data)
end_kmeans = time.time()

start_hierarchical = time.time()
hierarchical_linkage = linkage(scaled_data, method="complete")
hierarchical_labels = fcluster(hierarchical_linkage, t=4, criterion="maxclust")
end_hierarchical = time.time()

print(f"K-Means Time Taken: {end_kmeans - start_kmeans:.2f} seconds")
print(f"Hierarchical Time Taken: {end_hierarchical - start_hierarchical:.2f} seconds")

print ( "\nScalability:\n")
# Scalability:
def test_clustering_scalability(data, clustering_features, sizes, n_clusters=4):
    results = []
    for size in sizes:
        # Sample a subset of the data
        subset = data.sample(n=size, random_state=42)

        # Impute missing values using the mean
        for feature in clustering_features:
            subset[feature] = subset[feature].fillna(subset[feature].mean())

        # Scale the subset data using StandardScaler
        scaler = StandardScaler()  # Initialize StandardScaler inside the loop
        scaled_subset = scaler.fit_transform(subset[clustering_features])

        # K-Means
        start_kmeans = time.time()
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        kmeans.fit(scaled_subset)
        end_kmeans = time.time()
        kmeans_time = end_kmeans - start_kmeans

        # Hierarchical
        start_hierarchical = time.time()
        hierarchical_linkage = linkage(scaled_subset, method="ward")
        hierarchical_labels = fcluster(hierarchical_linkage, t=n_clusters, criterion="maxclust")
        end_hierarchical = time.time()
        hierarchical_time = end_hierarchical - start_hierarchical

        results.append( {"Size": size,"KMeans Time": kmeans_time, "Hierarchical Time": hierarchical_time,})
    return pd.DataFrame(results)

# Define dataset sizes to test
sizes = [10, 250, 500, 750]

scaler = StandardScaler()
scalability_results = test_clustering_scalability(df, clustering_features, sizes)

print(scalability_results)

print ( "\nInterpretability:\n")
# Interpretability
hierarchical_cluster_summary = new_cleaned_df.groupby("Hierarchical Cluster")[
    clustering_features ].mean()
cluster_summary = new_cleaned_df.groupby("Cluster")[clustering_features].mean()

print("\nCluster Summary (Hierarchical Clustering):\n", hierarchical_cluster_summary)
print("\nCluster Summary (K-Means):\n", cluster_summary)

